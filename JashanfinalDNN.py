#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from PIL import Image, UnidentifiedImageError
from sklearn.preprocessing import LabelEncoder
import seaborn as sns


# In[2]:


df = pd.read_json('photos.json', lines=True)


# In[3]:


df.head(5)


# As we can see above we have total of 5 labels or in other words we have 5 classes to predict.
# Lets take deeper look at the 5 classes

# In[4]:


df['label'].isnull().sum()      # checking if we have null values in label


# In[5]:


df['label'].unique()   # looking at all the classes


# Before we move forward we need to convert the string to interger in the label column

# In[6]:


label_encoder = LabelEncoder()
df['encoded_label'] = label_encoder.fit_transform(df['label'])


# In[7]:


df.head()


# ### Visualizing the Labels to understand the distribution 

# In[9]:


plt.figure(figsize=(10, 5))
sns.countplot(x='label', data=df)
plt.title('Distribution of Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


# In[10]:


#  Percentage of each label across all dataset
label_fractions = df['label'].value_counts(normalize=True)*100
print("Percentage of each label across all dataset:")
print(label_fractions)


# 1. **Function to Load and Preprocess Images:** The `load_images` function takes a DataFrame `data` and an image folder path `image_folder`. It iterates through the DataFrame, loading each image (up to 5000), resizes it to 128x128 pixels, normalizes the pixel values (scales them to a range of 0 to 1), and appends the image along with its corresponding label (`encoded_label`) to separate lists.
# 
# 2. **Handling Image Paths:** For each row, the `photo_id` is extracted, and the full image path is generated by joining the folder path with the `photo_id` and appending the `.jpg` extension.
# 
# 3. **Data Return as NumPy Arrays:** After processing the images and labels, they are converted into NumPy arrays for further use in model training. The function returns `X` (array of images) and `y` (array of labels) as output.

# In[11]:


from tensorflow.keras.preprocessing.image import load_img, img_to_array

def load_images(data, image_folder):
    images = []
    labels = []
    for index, row in data.iterrows():
        if len(images) >= 5000:      
            break
        try:
            photo_id = row['photo_id']
            label = row['encoded_label']
            image_path = os.path.join(image_folder, f"{photo_id}.jpg")
            image = load_img(image_path, target_size=(128, 128))  # Resizing the image
            image = img_to_array(image) / 255.0  # Normalizing the image
            images.append(image)
            labels.append(label)
        except:
            pass
    return np.array(images), np.array(labels)

image_folder = 'photos/'
X, y = load_images(df, image_folder)


# 1. **Stratified Split of Data:** The code uses StratifiedShuffleSplit to divide the dataset into training and test sets, ensuring that both sets have the same class distribution, which is useful for handling imbalanced datasets. It splits 20% of the data for testing and repeats this process 1 times.
# 
# 2. **Class Distribution Check:** After splitting, the class distributions in both the training and test sets are verified using unique, confirming that the stratified split has maintained the same proportion of labels in both sets.
# 

# In[14]:


print("Shape of X:", X.shape)
print("Shape of y:", y.shape)


# In[15]:


print("y array:", y)
print("Length of y:", len(y))


# In[16]:


from sklearn.model_selection import StratifiedShuffleSplit

# StratifiedShuffleSplit object
strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Perform stratified sampling
for train_idx, test_idx in strat_split.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]


# In[17]:


# Print the shape of the stratified train and test sets
print("Training set size:", X_train.shape, y_train.shape)
print("Test set size:", X_test.shape, y_test.shape)

# checking if the classes are stratified properly
unique, counts_train = np.unique(y_train, return_counts=True)
unique, counts_test = np.unique(y_test, return_counts=True)
print("Training set class distribution:", dict(zip(unique, counts_train)))
print("Test set class distribution:", dict(zip(unique, counts_test)))


# In[18]:


# Function to display images
def display_images(images, labels, num_images=20):
    plt.figure(figsize=(15, 10))
    
    for i in range(num_images):
        plt.subplot(4, 5, i + 1)  # Adjust the grid size based on num_images
        plt.imshow(images[i])      # Display the image
        plt.title(f'Label: {labels[i]}')  # Show the label as title
        plt.axis('off')            # Turn off axis
    
    plt.tight_layout()
    plt.show()

# Call the function to display 20 images
display_images(X_train, y_train, num_images=20)


# ## Perform the DNN ##
# Build and Compile the DNN Model

# In[20]:


from tensorflow.keras import Sequential
from tensorflow.keras import layers  


# In[21]:


from tensorflow.keras.layers import Dense, Input
model = Sequential([
    Input(shape=(10,), name='input_layer'),  # Explicit input layer
    Dense(128, activation='relu', name='hidden_layer_1'),  # First hidden layer
    Dense(64, activation='relu', name='hidden_layer_2'),   # Second hidden layer
    Dense(32, activation='relu', name='hidden_layer_3'),   # Third hidden layer
    Dense(16, activation='relu', name='hidden_layer_4'),   # Fourth hidden layer
    Dense(8, activation='relu', name='hidden_layer_5'),    # Fifth hidden layer
    Dense(4, activation='relu', name='hidden_layer_6'),    # Sixth hidden layer
    Dense(2, activation='relu', name='hidden_layer_7'),    # Seventh hidden layer
    Dense(1, activation='sigmoid', name='output_layer')     # Output layer
])
print("Created a Sequential Model with 8 layers.")  #(create the output/print)
model.summary()
print("MODEL OF 8 LAYERS.")   #(Model Overview for 8 layers )
  


# In[22]:


import tensorflow as tf
from tensorflow import keras
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu", name="layer1"),
        layers.Dense(3, activation="relu", name="layer2"),
        layers.Dense(4, name="layer3"),
    ]
)
x = tf.ones((3, 3))  
y = model(x)  
print(y)  # (Create the tensorflow of 3*4)


# In[23]:


# Early stopping callback
from tensorflow.keras import callbacks 
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) 


# This is one of the important library which is import because some the epochs run to maximum numbers which might effects the system

# **Now train the Model with Validation Split**

# In[25]:


def create_dnn_model(input_shape):
    model = keras.Sequential([
        layers.InputLayer(shape=(input_shape,)),  # Use shape instead of input_shape
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

model = create_dnn_model(X_train.shape[1])
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
print('Model created and compiled. Ready for training.')
model.summary()


# In[38]:


from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Define a function to create the model
def create_dnn_model(input_shape):
    model = keras.Sequential([
        layers.InputLayer(shape=(input_shape,)),  # Use the flattened input shape
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1, activation='sigmoid')  # Binary classification output
    ])
    return model

# Flatten the input data (assuming you're working with image data)
X_train_flat = X_train.reshape(X_train.shape[0], -1)  # Flatten each image to a 1D array
X_test_flat = X_test.reshape(X_test.shape[0], -1)    # Flatten test data similarly

# Create the model with the flattened input shape
model = create_dnn_model(X_train_flat.shape[1])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

print('Model created and compiled. Ready for training.')

# Set up early stopping to prevent overfitting
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Train the model with the flattened data
history = model.fit(
    X_train_flat,  # Use the flattened data
    y_train,
    epochs=50,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)

print(f'Test accuracy: {test_accuracy:.4f}')
print(f'Test loss: {test_loss:.4f}')

# Plot training history
plt.figure(figsize=(12, 4))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Accuracy plot
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()  # Show the plots


# In[26]:


model = keras.Sequential()
model.add(keras.Input(shape=(4,))) 
model.add(layers.Dense(2, activation="relu"))


# In[30]:


X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

def create_dnn_model(input_shape):
    model = keras.Sequential([
        layers.InputLayer(shape=(input_shape,)),  # Adjust input shape to match the flattened data
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(1, activation='sigmoid')  # Assuming binary classification
    ])
    return model

model = create_dnn_model(X_train_flat.shape[1])
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
history = model.fit(
    X_train_flat,  # Use the flattened data
    y_train,
    epochs=50,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)
test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)

print(f'Test accuracy: {test_accuracy:.4f}')
print(f'Test loss: {test_loss:.4f}')


# *code flatten the  image data, converting 3D image arrays into 1D vectors .* 
# *The model is defined with multiple hidden layers using ReLU activation and dropout to prevent overfitting. *
# 
# *The model is trained on the flattened data and using the  mini-batche with the validation data and early stopping so hepls to save from overfitting and increase the performance and prevent overfitting. After 50 epochs  model is evaluated on test data to measure its accuracy and loss.*

# **HYPERPARAMETER TUNING**

# In[32]:


pip install keras-tuner


# In[33]:


from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

def build_hyper_model(hp):
    model = keras.Sequential()
    model.add(layers.InputLayer(input_shape=(X_train.shape[1],)))
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    model.add(layers.Dense(units=hp_units, activation='relu'))
    hp_dropout = hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4])
    model.add(layers.Dropout(rate=hp_dropout))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model


random_search = RandomSearch(build_hyper_model,
                             objective='val_accuracy',
                             max_trials=5,
                             executions_per_trial=1,
                             directory='my_dir',
                             project_name='bank_hyperparam_tuning')

random_search.search(X_train, y_train,
                     epochs=10,
                     validation_split=0.2,
                     callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)])

best_hps = random_search.get_best_hyperparameters(num_trials=1)[0]

print('The optimal number of units in the dense layer is:', best_hps.get('units'))
print('The optimal learning rate is:', best_hps.get('learning_rate'))
print('The optimal dropout rate is:', best_hps.get('dropout_rate'))


# In[ ]:


* The hyperparameter search runs for a maximum of 5 trials, each executing the model training for up to 10 epochs


# In[36]:


from kerastuner.tuners import RandomSearch
from tensorflow import keras
from tensorflow.keras import layers

X_train_flat = X_train.reshape(X_train.shape[0], -1)  
X_test_flat = X_test.reshape(X_test.shape[0], -1)   


def build_hyper_model(hp):
    model = keras.Sequential()
    model.add(layers.InputLayer(input_shape=(X_train_flat.shape[1],))) 

    for i in range(hp.Int('num_layers', 1, 5)):
        hp_units = hp.Int('units_' + str(i), min_value=64, max_value=512, step=64)
        model.add(layers.Dense(units=hp_units, activation='relu'))
        
        hp_dropout = hp.Choice('dropout_rate_' + str(i), values=[0.1, 0.2, 0.3])
        model.add(layers.Dropout(rate=hp_dropout))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model
random_search = RandomSearch(
    build_hyper_model,
    objective='val_accuracy',  
    max_trials=5,              
    executions_per_trial=1,     
    directory='my_dir',        
    project_name='dnn_hyperparam_tuning'
)

random_search.search(X_train_flat, y_train,
                     epochs=20,                   
                     validation_split=0.2,        
                     callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])  

best_hps = random_search.get_best_hyperparameters(num_trials=1)[0]

print('The optimal number of layers is:', best_hps.get('num_layers'))
print('The optimal number of units in the dense layer is:', 
      [best_hps.get('units_' + str(i)) for i in range(best_hps.get('num_layers'))])
print('The optimal learning rate is:', best_hps.get('learning_rate'))
print('The optimal dropout rates are:', 
      [best_hps.get('dropout_rate_' + str(i)) for i in range(best_hps.get('num_layers'))])


#  *Image data and tunes the number of layers, units per layer, dropout rates, and learning rates. The model is compiled with binary cross-entropy loss and accuracy as the evaluation metric.*
# 
# *The random search runs 5 trials, each trained for 20 epochs with early stopping based on validation loss.* 
# After the search 
# *Best hyperparameters =5
# *Number of layers =[256, 64, 64, 64, 64]
# *Dropout rates= [0.1, 0.1, 0.1, 0.1, 0.1]
# *learning rate=0.001
# 

# In[40]:


test_loss, test_accuracy = model.evaluate(X_test_flat, y_test)
print(f"Test Loss: {test_loss:.3f}, Test Accuracy: {test_accuracy:.3f}")


# *Find the loss test set*

# In[4]:


# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.callbacks import EarlyStopping

# Load your dataset from JSON
data = pd.read_json('photos.json', lines=True)

# Check the dataset structure
print(data.head())

# Use TF-IDF to transform the 'caption' text column into numeric features
tfidf = TfidfVectorizer(max_features=1000)  # Increase max_features to capture more information
X = tfidf.fit_transform(data['caption'].fillna('')).toarray()  # Convert text to numeric form

# Encode the 'label' column to numeric values
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data['label'].fillna(''))  # Encoding categorical labels

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build your deep learning model
model = Sequential()

# Add L2 regularization and dropout
model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],),
                kernel_regularizer=l2(0.001)))  # L2 regularization with lambda=0.001
model.add(Dropout(0.5))  # Dropout to prevent overfitting

model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))  # L2 regularization
model.add(Dropout(0.5))

model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # L2 regularization
model.add(Dropout(0.3))

# Output layer for multiclass classification
model.add(Dense(len(np.unique(y)), activation='softmax'))

# Compile the model (categorical_crossentropy for multiclass classification)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Implement Early Stopping to stop training if the model stops improving
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)

# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print the results
print(f"Test Accuracy: {test_acc}")
print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))



# In[43]:


# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer

data = pd.read_json('photos.json', lines=True)


print(data.head())


tfidf = TfidfVectorizer(max_features=500)  
X = tfidf.fit_transform(data['caption'].fillna('')).toarray()  


label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data['label'].fillna(''))  # Encoding categorical labels


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


model = Sequential()


model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))


model.add(Dropout(0.3))


model.add(Dense(64, activation='relu'))


model.add(Dropout(0.3))

# Output layer for multiclass classification (use softmax activation for more than 2 classes)
model.add(Dense(len(np.unique(y)), activation='softmax'))

# Compile the model (categorical_crossentropy for multiclass classification)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)

# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print the results
print(f"Test Accuracy: {test_acc}")
print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))


# In[44]:


# Import necessary libraries

from tensorflow.keras.regularizers import l2


# Add L2 regularization and dropout
model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001))) 
model.add(Dropout(0.5))  

model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))  
model.add(Dropout(0.5))

model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  
model.add(Dropout(0.3))


model.add(Dense(len(np.unique(y)), activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

test_loss, test_acc = model.evaluate(X_test, y_test)

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

print(f"Test Accuracy: {test_acc}")
print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))


# * L2 regularization (kernel_regularizer=l2(0.001)) prevents overfitting by penalizing large weights. *
# * The combination of L2 and dropout improves the modelâ€™s generalization ability *

# In[1]:


pip install shap


# In[2]:


# Step 5: Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix using Seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()


# In[ ]:




